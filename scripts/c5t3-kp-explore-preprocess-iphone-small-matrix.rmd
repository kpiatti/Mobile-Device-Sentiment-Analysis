---
title: "Preprocess & Explore iPhone Small Matrix"
author: "Katherine Piatti"
date: "4/29/2021"
output: 
 html_document:
 theme: united
 toc: TRUE
 toc_depth: 2
 toc_float: TRUE
 highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# SETUP ##########################################
  
## Project Overview ##############################

The goal of this project is to use text mining to gather data on sentiments towards iPhone and Samsung Galaxy mobile devices from archived webpages from the CommonCrawl archive.

In this notebook I will be analyzing and modeling a small matrix of data just concerning sentiment toward iPhone devices. 


## Libraries & Renv #####################################

  - I ran renv::init 2021-39-04 2:08PM
  
```{r}
#load pkgs
library(tidyverse)
library(here)
library(janitor)
library(corrr)
library(GGally)

library(caret)
library(doParallel)
```


## Import Data ###################################

```{r}
#read in iphone small matrix
iphone_df_orig <- read.csv(here("data", "iphone_smallmatrix_labeled_8d.csv")) 

# define copy of original data, clean var names, & remove any empty columns or rows
iphone_df <- iphone_df_orig %>%
  clean_names() %>% 
  remove_empty()
```



# DATA WRANGLING ################################

```{r}
#get overview of data
glimpse(iphone_df)
```

The raw dataset contains **12, 973** rows (observations) and 59 columns (variables).


## Remove Irrelevant Variables ######################

Many of the 59 variables in the raw dataset only contain data about mobile devices other than the iPhone (e.g. Nokia, Samsung) and thus are irrelevant for current purposes. So my first order of business is to remove all these irrelevant variables.

  - I know from examining the supplemental information that came with the datatset (Helio_Sentiment_Analysis_Matrix_Detail.csv) that the names of **all** and **only** the relevant (iPhone) variables begin with either "iphone" or "ios". So I will select and keep only columns starting with either of those strings. 

```{r}
#select only columns that start with iphone or ios
iphone_df <- iphone_df %>% 
  select(starts_with(c("iphone", "ios"))) 

glimpse(iphone_df)
```

Now the dataset contains only **15** variables.


## Missing Values #################################

```{r}
#check for missing values
iphone_df %>% 
  is.na() %>% 
  sum()
```
There are no explicit NAs in the data. However, since I don't know the protocol for entering missing values in this dataset, I also want to check for other values that might have been used to represent missing data (e.g 999).

```{r}
# gat all the unique values in each var
iphone_df %>% 
  map(unique)
```
*None of the values appear to be representing missing values. 


## Duplicates ###################################

Because the website addresses and no other uniquely identifying information has been retained, there is no way to definitively identify duplicate observations--it's possible observations with the same values for all variables are from genuinely different websites that just happen to be identical on all measured datapoints (e.g. mention the iphone camera the exact same number of times).

However, the probability of genuinely different websites having the exact same values across 15 measures may be very improbable, and we know that the one piece of content (e.g. a review) is commonly published/re-published on multiple different websites, thus identical rows may indeed bed genuine duplicates. 

> *Note: In a real scenario (as opposed to a class project) I would avoid this problem by changing the data collection protocol to capture aned retain information that could be used to differentiate genuine from ersatz duplicates.*

```{r}
#get number of dupe rows
iphone_df %>% 
  duplicated() %>% 
  sum()
```


```{r}
#keep only distinct rows
iphone_df %>% 
  distinct() %>% 
  glimpse()

iphone_distinct <- iphone_df %>% 
  distinct()
```

# DATA EXPLORATION ######################################


## Feature Correlations #################################

```{r}
#get var correlations > 0.7 abd -0.7
(corr <- iphone_df %>% 
  correlate() %>% 
  shave() %>% 
  stretch() %>% 
  filter(abs(r) > 0.7) %>% 
  arrange(desc(r)))
```

It's surprising that variables representing positive and negative sentiment towards a particular iphone feature (e.g. display) are among those with the highest positive correlations. The variables in question record the number of positive or negative (respectively) words or expressions near the feature word (e.g. "display") or it's synonyms.

Thus, the high positive correlation between these variables suggests as the number of positive words about the feature increases so too does the number of negative words about the feature. 

It's also notable that none of the highest correlations are with our target variable--iphonesentiment.

```{r}
iphone_df %>% 
  select(c(iphonedispos, iphonedisneg, iphonedisunc, iphonesentiment)) %>% 
  ggpairs()
```

```{r}
iphone_df %>% 
  select(iphoneperpos, iphoneperneg, iphoneperunc, iphonesentiment) %>% 
  ggpairs()
```



## Feature Variance ######################################

```{r}
#look for vars with near zero variance
iphone_df %>% 
  nearZeroVar(saveMetrics = TRUE) %>% 
  arrange(nzv)
```

Since there's near zero variance in all the variables representing sentiment toward the iphone OS, those 4 variables are good candidates for removal. 


## Recursive Feature Selection ##################################

```{r}
set.seed(123)

#define 3,000 obs. sample
iphone_sample <-  slice_sample(.data = iphone_df,
             n = 3000, 
             replace = FALSE)


#move target variable to end
iphone_sample <- iphone_sample %>% 
  relocate(iphonesentiment,
           .after = iosperunc)

#set up rfe control with randomn forest and repeated CV
ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5, 
                   verbose = FALSE)
```


```{r}
set.seed(999)

rfe_results <- rfe(iphone_sample[, 1:14], 
                   iphone_sample$iphonesentiment, 
                   rfeControl = ctrl)

#get results
rfe_results
```

The model with all 14 variable was selected as performing best on the sample data. 


```{r}
plot(rfe_results)
```


## Histograms ################

```{r}
iphone_df %>% 
  map(hist)
```

All of the independent variables have very skewed distributions swamped by zeros. Let's look at variance.



#  DV Data Type ################################################

The dependent variable (iphonesentiment) is set as data type: integer because it contains values 1-5. However, those values correspond to different overall sentiment toward the iphone.
    
    * 0 = sentiment unclear
    * 1 = negative
    * 2 = somewhat negative
    * 3 = neutral
    * 4 = somewhat positive
    * 5 = positive

Given this, the data type of iphonesentiment should be changed to factor.

```{r}
iphone_df$iphonesentiment <-  as_factor(iphone_df$iphonesentiment)

class(iphone_df$iphonesentiment)
```

```{r}
iphone_df$iphonesentiment <- fct_recode(iphone_df$iphonesentiment,
             unclear = "0",
             neg = '1',
             vneg = '2',
             neutral = '3',
             pos = '4',
             vpos = '5')

head(iphone_df$iphonesentiment)
```


## Parallel Processing ###########################

Because the modeling process can be computationally taxing and my machine is not very powerful, to speed things up, I'm setting up parallel processing that enables R to use multiple cores to perform computations.

```{r}
#get numberr of cores on my machine
detectCores()

# create cluster
ppcluster <- makeCluster(3)

#register cluster
registerDoParallel(ppcluster)

# get number of cores now assigned to R
getDoParWorkers()
```
When done working in R for the day, don't forget to stopCluster(ppcluster)







